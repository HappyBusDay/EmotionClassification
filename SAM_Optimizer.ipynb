{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Prepare"
      ],
      "metadata": {
        "id": "GJ7AwWjUeoIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "vWE2P3t7eQpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import random\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModel\n",
        "from torch.optim import AdamW\n",
        "import torch.optim as optim\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "metadata": {
        "id": "eqHF1wdyeO2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Optimizer"
      ],
      "metadata": {
        "id": "c9KImDO5eqkN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AXS3bxQeN7_"
      },
      "outputs": [],
      "source": [
        "class SAM(torch.optim.Optimizer):\n",
        "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
        "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
        "\n",
        "        defaults = dict(rho=rho, **kwargs)\n",
        "        super(SAM, self).__init__(params, defaults)\n",
        "\n",
        "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
        "        self.param_groups = self.base_optimizer.param_groups\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def first_step(self, zero_grad=False):\n",
        "        grad_norm = self._grad_norm()\n",
        "        for group in self.param_groups:\n",
        "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
        "\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                e_w = p.grad * scale.to(p)\n",
        "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
        "                self.state[p][\"e_w\"] = e_w\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def second_step(self, zero_grad=False):\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                if p.grad is None: continue\n",
        "                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n",
        "\n",
        "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
        "\n",
        "        if zero_grad: self.zero_grad()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
        "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
        "\n",
        "        self.first_step(zero_grad=True)\n",
        "        closure()\n",
        "        self.second_step()\n",
        "\n",
        "    def _grad_norm(self):\n",
        "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
        "        norm = torch.norm(\n",
        "                    torch.stack([\n",
        "                        p.grad.norm(p=2).to(shared_device)\n",
        "                        for group in self.param_groups for p in group[\"params\"]\n",
        "                        if p.grad is not None\n",
        "                    ]),\n",
        "                    p=2\n",
        "               )\n",
        "        return norm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train"
      ],
      "metadata": {
        "id": "Omf2UZLdesZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SAM\n",
        "\n",
        "def train(model, optimizer, train_loader, test_loader, device):\n",
        "\n",
        "    # model.load_state_dict(torch.load(model_path))\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "    best_score = 0\n",
        "    best_model = \"None\"\n",
        "\n",
        "    epoch_step = 0\n",
        "\n",
        "    for epoch_num in range(CFG[\"EPOCHS\"]):\n",
        "        \n",
        "        model.train()\n",
        "\n",
        "        train_loss = []\n",
        "\n",
        "        for input_ids, attention_mask, train_label in tqdm(train_loader):\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            train_label = train_label.to(device)\n",
        "            input_id = input_ids.to(device)\n",
        "            mask = attention_mask.to(device)\n",
        "\n",
        "            output = model(input_id, mask)     \n",
        "            \n",
        "\n",
        "            loss1 = criterion(output, train_label.long()) \n",
        "            loss1.backward(retain_graph=True)\n",
        "            optimizer.first_step(zero_grad=True)\n",
        "            \n",
        "            loss2 = criterion(model(input_id, mask), train_label.long())\n",
        "            loss2.backward()  \n",
        "            optimizer.second_step(zero_grad=True)\n",
        "\n",
        "            train_loss.append(loss2.item())\n",
        "            \n",
        "        epoch_step += 1\n",
        "\n",
        "        val_loss, val_score = validation(model, criterion, test_loader, device)\n",
        "\n",
        "        # scheduler.step(float(np.mean(val_loss)))\n",
        "\n",
        "        print(f'Epoch [{epoch_step}], Train Loss : [{np.mean(train_loss) :.5f}] Val Loss : [{np.mean(val_loss) :.5f}] Val F1 Score : [{val_score:.5f}]')\n",
        "\n",
        "        model_saved_path = './path' + str(epoch_step) + '.pt'\n",
        "\n",
        "        torch.save(model.state_dict(), model_saved_path)\n",
        "\n",
        "        if best_score < val_score:\n",
        "            best_model = model\n",
        "            best_score = val_score\n",
        "        \n",
        "    return best_model                         "
      ],
      "metadata": {
        "id": "-GpQsmg-eWUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SAM\n",
        "model = BaseModel()\n",
        "base_optimizer = torch.optim.AdamW  \n",
        "optimizer = SAM(model.parameters(), base_optimizer, lr=CFG['LEARNING_RATE'])\n",
        "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.1, patience = 1, threshold = 1e-3, verbose = True)\n",
        "model.eval()\n",
        "\n",
        "infer_model = train(model, optimizer, train_dataloader, val_dataloader, device)"
      ],
      "metadata": {
        "id": "pN_w0JiLeuV7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}